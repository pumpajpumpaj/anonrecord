<!DOCTYPE html>
<html>
<head>
  <title>Recorder</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <style>
    canvas, video {
      position: absolute;
      top: 0; left: 0;
    }
    #container {
      position: relative;
      width: 640px;
      height: 480px;
    }
  </style>
</head>
<body>

<h2>Face Detection with Live Camera</h2>
<label for="cameraSelect">Choose camera:</label>
<select id="cameraSelect"></select>
<div id="container">
  <video id="video" autoplay muted playsinline width="640" height="480"></video>
  <canvas id="canvas" width="640" height="480"></canvas>
</div>
<div>
  <button id="startBtn">Start Recording</button>
  <button id="stopBtn" disabled>Stop Recording</button>
  <a id="downloadLink" style="display:none" download="censored-video.webm">Download video</a>
</div>


  <script>
    
    function loadOpenCV() {
      return new Promise((resolve, reject) => {
        const script = document.createElement('script');
        script.src = 'https://docs.opencv.org/4.5.5/opencv.js';
        script.async = true;
        script.onload = () => {
          cv['onRuntimeInitialized'] = () => resolve();
        };
        script.onerror = reject;
        document.body.appendChild(script);
      });
    }
let mediaRecorder = null;
let recordedChunks = [];
let micStream = null;

    let previousFaces = [];
const MAX_MISSING_FRAMES = 13;
function computeIoU(boxA, boxB) {
  const xA = Math.max(boxA[0], boxB[0]);
  const yA = Math.max(boxA[1], boxB[1]);
  const xB = Math.min(boxA[2], boxB[2]);
  const yB = Math.min(boxA[3], boxB[3]);

  const interArea = Math.max(0, xB - xA) * Math.max(0, yB - yA);
  if (interArea === 0) return 0;

  const boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1]);
  const boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1]);

  return interArea / (boxAArea + boxBArea - interArea);
}

    let session = null;
    let labels = [];
    const THRESHOLD = 0.7;
    const IOU_THRESHOLD = 0.3;
    const MODEL_URL = "ultraface.onnx";

    
    async function setupRecordingWithAudio(canvas) {
  const startBtn = document.getElementById('startBtn');
  const stopBtn = document.getElementById('stopBtn');
  const downloadLink = document.getElementById('downloadLink');

  
  const canvasStream = canvas.captureStream(25); 

  
  micStream = await navigator.mediaDevices.getUserMedia({ audio: true });

  
  const combinedStream = new MediaStream([
    ...canvasStream.getVideoTracks(),
    ...micStream.getAudioTracks()
  ]);

  
  mediaRecorder = new MediaRecorder(combinedStream, { mimeType: 'video/webm' });

  mediaRecorder.ondataavailable = e => {
    if (e.data.size > 0) recordedChunks.push(e.data);
  };

  mediaRecorder.onstop = () => {
    const blob = new Blob(recordedChunks, { type: 'video/webm' });
    recordedChunks = [];

    const url = URL.createObjectURL(blob);
    downloadLink.href = url;
    downloadLink.style.display = 'inline-block';
    downloadLink.innerText = 'Download video';

    
    micStream.getTracks().forEach(track => track.stop());
  };

  startBtn.onclick = () => {
    recordedChunks = [];
    mediaRecorder.start();
    startBtn.disabled = true;
    stopBtn.disabled = false;
    downloadLink.style.display = 'none';
  };

  stopBtn.onclick = () => {
    mediaRecorder.stop();
    startBtn.disabled = false;
    stopBtn.disabled = true;
  };
}

    async function loadLabels() {
      labels = ['background', 'face'];
    }

    function softNMS(boxes, scores, iouThreshold) {
      const picked = [];
      const areas = boxes.map(b => (b[2] - b[0]) * (b[3] - b[1]));
      const order = scores.map((s, i) => [s, i]).sort((a, b) => b[0] - a[0]).map(e => e[1]);

      while (order.length > 0) {
        const i = order.shift();
        picked.push(i);
        for (let j = order.length - 1; j >= 0; j--) {
          const o = order[j];
          const xx1 = Math.max(boxes[i][0], boxes[o][0]);
          const yy1 = Math.max(boxes[i][1], boxes[o][1]);
          const xx2 = Math.min(boxes[i][2], boxes[o][2]);
          const yy2 = Math.min(boxes[i][3], boxes[o][3]);
          const w = Math.max(0, xx2 - xx1);
          const h = Math.max(0, yy2 - yy1);
          const inter = w * h;
          const ovr = inter / (areas[i] + areas[o] - inter);
          if (ovr > iouThreshold) order.splice(j, 1);
        }
      }
      return picked;
    }

    function preprocess(mat) {
      let dst = new cv.Mat();
      cv.cvtColor(mat, dst, cv.COLOR_RGBA2RGB);
      cv.resize(dst, dst, new cv.Size(320, 240));
      let imgData = dst.data;
      let float32 = new Float32Array(1 * 3 * 240 * 320);
      const mean = [127, 127, 127];

      for (let i = 0; i < 240; i++) {
        for (let j = 0; j < 320; j++) {
          for (let c = 0; c < 3; c++) {
            let val = imgData[(i * 320 + j) * 3 + c];
            float32[c * 240 * 320 + i * 320 + j] = (val - mean[c]) / 128.0;
          }
        }
      }

      dst.delete();
      return new ort.Tensor("float32", float32, [1, 3, 240, 320]);
    }

async function runInference(frameMat, ctx) {
  const inputTensor = preprocess(frameMat);
  const output = await session.run({ [session.inputNames[0]]: inputTensor });
  const confidences = output[session.outputNames[0]].data;
  const boxes = output[session.outputNames[1]].data;

  const confArr = Array.from(confidences);
  const boxArr = Array.from(boxes);
  const numAnchors = confidences.length / labels.length;

  const results = [];
  for (let i = 0; i < numAnchors; i++) {
    const score = confArr[i * labels.length + 1];
    if (score > THRESHOLD) {
      const box = [
        boxArr[i * 4 + 0],
        boxArr[i * 4 + 1],
        boxArr[i * 4 + 2],
        boxArr[i * 4 + 3]
      ];
      results.push({ box, score });
    }
  }

  const picked = softNMS(results.map(r => r.box), results.map(r => r.score), IOU_THRESHOLD);
  const currentFaces = [];

  for (let i of picked) {
    let { box } = results[i];
    let [x1, y1, x2, y2] = box.map((v, idx) => idx % 2 === 0 ? v * frameMat.cols : v * frameMat.rows);
    x1 = Math.max(0, Math.floor(x1));
    y1 = Math.max(0, Math.floor(y1));
    x2 = Math.min(frameMat.cols, Math.floor(x2));
    y2 = Math.min(frameMat.rows, Math.floor(y2));

    currentFaces.push({ x1, y1, x2, y2 });

    let rect = new cv.Rect(x1, y1, x2 - x1, y2 - y1);
    let faceROI = frameMat.roi(rect);
    let mean = cv.mean(faceROI);
    faceROI.delete();

    let center = new cv.Point(x1 + (x2 - x1) / 2, y1 + (y2 - y1) / 2);
    let axes = new cv.Size((x2 - x1) / 2, (y2 - y1) / 2);
    cv.ellipse(frameMat, center, axes, 0, 0, 360, new cv.Scalar(mean[0], mean[1], mean[2], 255), cv.FILLED);
  }

  
  const newTracked = [];

  for (const face of previousFaces) {
    let matched = false;
    for (const curr of currentFaces) {
      const iou = computeIoU([face.x1, face.y1, face.x2, face.y2], [curr.x1, curr.y1, curr.x2, curr.y2]);
      if (iou > 0.4) {
        matched = true;
        break;
      }
    }

    if (!matched) {
      face.missingFrames++;
      if (face.missingFrames <= MAX_MISSING_FRAMES) {
        
        let x1 = face.x1, y1 = face.y1, x2 = face.x2, y2 = face.y2;
        let center = new cv.Point(x1 + (x2 - x1) / 2, y1 + (y2 - y1) / 2);
        let axes = new cv.Size((x2 - x1) / 2, (y2 - y1) / 2);
        let mean = face.mean;
        cv.ellipse(frameMat, center, axes, 0, 0, 360, new cv.Scalar(mean[0], mean[1], mean[2], 255), cv.FILLED);
        newTracked.push(face);
      }
    }
  }

  
  for (const curr of currentFaces) {
    let rect = new cv.Rect(curr.x1, curr.y1, curr.x2 - curr.x1, curr.y2 - curr.y1);
    let roi = frameMat.roi(rect);
    let mean = cv.mean(roi);
    roi.delete();

    newTracked.push({
      x1: curr.x1,
      y1: curr.y1,
      x2: curr.x2,
      y2: curr.y2,
      mean,
      missingFrames: 0
    });
  }

  previousFaces = newTracked;

  
  cv.imshow(ctx.canvas, frameMat);
}



let currentStream = null;

async function getVideoDevices() {
  const devices = await navigator.mediaDevices.enumerateDevices();
  return devices.filter(d => d.kind === 'videoinput');
}

async function populateCameraList() {
  const cameraSelect = document.getElementById('cameraSelect');
  const devices = await getVideoDevices();
  cameraSelect.innerHTML = '';
  devices.forEach((device, index) => {
    const option = document.createElement('option');
    option.value = device.deviceId;
    option.text = device.label || `Camera ${index + 1}`;
    cameraSelect.appendChild(option);
  });
  return devices;
}

// async function startCameraById(deviceId) {
//   if (currentStream) {
//     currentStream.getTracks().forEach(t => t.stop());
//   }
// 
//   const constraints = {
//     video: {
//       deviceId: { exact: deviceId },
//       width: { ideal: 640 },
//       height: { ideal: 480 },
//     }
//   };
// 
//   const stream = await navigator.mediaDevices.getUserMedia(constraints);
//   currentStream = stream;
// 
//   const video = document.getElementById('video');
//   video.srcObject = stream;
// 
//   return new Promise(resolve => {
//     video.onloadedmetadata = () => {
//       video.play();
//       resolve();
//     };
//   });
// }
async function startCameraById(deviceId) {
  if (currentStream) {
    currentStream.getTracks().forEach(t => t.stop());
  }

  const constraints = {
video: {
  deviceId: { exact: deviceId },
  width: { ideal: 1280 },
  height: { ideal: 720 }
},

    audio: false
  };

  const stream = await navigator.mediaDevices.getUserMedia(constraints);
  currentStream = stream;

  const video = document.getElementById('video');
  video.srcObject = stream;

  await new Promise(resolve => {
    video.onloadedmetadata = () => resolve();
  });

  // Set canvas size to match video native resolution:
  const canvas = document.getElementById('canvas');
  canvas.width = video.width;
  canvas.height = video.height;

  return;
}


    async function mainLoop(video, canvas) {
      const ctx = canvas.getContext('2d');
      const src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
      const cap = new cv.VideoCapture(video);

      async function loop() {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        cap.read(src);
        await runInference(src, ctx);
        requestAnimationFrame(loop);
      }

      loop();
    }

async function init() {
  await loadOpenCV();
  await loadLabels();
  session = await ort.InferenceSession.create(MODEL_URL);

  const canvas = document.getElementById('canvas');
  const video = document.getElementById('video');
  const cameraSelect = document.getElementById('cameraSelect');

  const devices = await populateCameraList();

  if (devices.length === 0) {
    alert("No video input devices found.");
    return;
  }

  await startCameraById(devices[0].deviceId); 
  await mainLoop(video, canvas);
  await setupRecordingWithAudio(document.getElementById('canvas'));

  cameraSelect.addEventListener('change', async (e) => {
    await startCameraById(e.target.value);
  });
}


    init();
  </script>
</body>
</html>
